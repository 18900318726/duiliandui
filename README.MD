# 对联队

### 项目名称
    对对联

### 实现原理：
    数据加载，对联数据集的特点是输入与输出等长。
    构造字典：为了将文字变成计算机能处理的id，我们需要构建一个字典。
    数据预处理：控制输出长度，使其与输入数据等长，方便交给模型处理。
    模型构建：构建seq2seq模型，将encode的编码作为decode的输入。
    模型训练：模型构建完成后，开始训练模型。模型预测：测试学习效果。
    简单说就是一个翻译模型，通过使用深度神经网络将一个语言序列翻译成另一种语言序列。

### 项目结构
 ![image](https://github.com/ButBueatiful/dotvim/raw/master/screenshots/vim-screenshot.jpg)<br>
 couplet        文件夹用于存放一些数据（训练数据和测试数据，以及词表数据）<br>
 models         文件夹用于存放训练阶段保存的模型<br>
 couplet.py     训练模型保存到models<br>
 seq2seq_attention  带有attantion机制的seq2seq模型<br>
 data_utils     数据处理<br>
 eval_function  bleu计算，用于评估训练阶段的模型效果<br>
 main       定义了model类型，里面包括了模型的输入，输出，重载保存，训练等对模型的操作，是整个项目的重点<br>
 Server    项目原型
 ### 相关知识
 ##### 循环神经网络RNN基本概念
 RNN可以用先前的状态预测后来的状态，能够记忆先前的状态，适合用来解决具有上下文关系的算法问题。
 ##### 输入输出不等长的多输入多输出的RNN结构——Seq2Seq模型
 seq2seq 是一个 Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。
 ##### RNN特殊类型——LSTM
 LSTM对有价值的信息进行记忆，放弃冗余记忆，从而减小学习难度。与RNN相比，LSTM的神经元还是基于输入X和上一级的隐藏层输出h来计算，只不过内部结构变了，也就是神经元的运算公式变了，而外部结构并没有任何变化，因此上面提及的RNN各种结构都能用LSTM来替换。
 ##### BLEU得分
 它是用来评估机器翻译跟专业人工翻译之间的对应关系，核心思想就是机器翻译越接近专业人工翻译，质量就越好，经过bleu算法得出的分数可以作为机器翻译质量的其中一个指标
